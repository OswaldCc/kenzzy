{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO7+EPinlRVpbvrDCTMysu/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OswaldCc/kenzzy/blob/main/kenzzy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF1WDDxsHCf5",
        "outputId": "fbaf7bc3-2bbd-4272-c635-3360e64a1fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 14.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 53.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 82.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugAjz-npyZEy",
        "outputId": "2ac5a72b-b4ea-4f24-cc8c-596dde948ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversation id: 8e9a0ce7-d574-4a93-b3be-5424f4cc0f1f \n",
            "user >> I feel a bit lonely today. Any advice? \n",
            "bot >> I'm here for you. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "input='I feel a bit lonely today. Any advice?'\n",
        "nlp = transformers.pipeline(\"conversational\", model=\"microsoft/DialoGPT-medium\",pad_token_id=50256)\n",
        "chat = nlp(transformers.Conversation(input))\n",
        "res = str(chat)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "# Instantiate the tokenizer and the model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "\n",
        "# Let's chat for 5 lines\n",
        "for step in range(5):\n",
        "    # Encode the new user input and add the eos_token\n",
        "    new_user_input = input(\">> User:\") + tokenizer.eos_token\n",
        "    new_user_input_ids = tf.constant(tokenizer.encode(new_user_input))[None, :]\n",
        "\n",
        "    # Append the new user input tokens to the chat history\n",
        "    if step > 0:\n",
        "        bot_input_ids = tf.concat([chat_history_ids, new_user_input_ids], axis=-1)\n",
        "    else:\n",
        "        bot_input_ids = new_user_input_ids\n",
        "\n",
        "    # Generate a response while limiting the total chat history to 1000 tokens\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, eos_token_id=tokenizer.eos_token_id)[:, bot_input_ids.shape[-1]:]\n",
        "\n",
        "    # Decode and pretty print the last output tokens from the bot\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[0], skip_special_tokens=True)))"
      ],
      "metadata": {
        "id": "xNihM8Sg4EG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "IwY8lvyYMcCE",
        "outputId": "86de1e44-6758-467d-a88a-2d3af04e6abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.25.0.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 2.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.5.2.221213-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 29.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Collecting types-pytz>=2022.1.1\n",
            "  Downloading types_pytz-2022.7.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=67dfb8bcb63b4691f1837cb25f65cf2918912f251d68a48691af7d19c3805101\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/92/33/6f57c7aae0b16875267999a50570e81f15eecec577ebe05a2e\n",
            "Successfully built openai\n",
            "Installing collected packages: types-pytz, pandas-stubs, openai\n",
            "Successfully installed openai-0.25.0 pandas-stubs-1.5.2.221213 types-pytz-2022.7.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set up the OpenAI API client with your API key\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "def generate_response(prompt):\n",
        "  # Use the OpenAI API to generate a response to the prompt\n",
        "  response = openai.Completion.create(\n",
        "      engine=\"text-ada-001\",\n",
        "      prompt=prompt,\n",
        "      temperature=0.9,\n",
        "      max_tokens=1024\n",
        "  )\n",
        "\n",
        "  # Return the generated response\n",
        "  return response\n",
        "\n",
        "# Define the identity and behavior of the bot\n",
        "\n",
        "\n",
        "# Use the identity and behavior of the bot to craft a prompt for the API\n",
        "prompt = \"hi\"\n",
        "\n",
        "# Generate a response to the prompt\n",
        "bot_response = generate_response(prompt)\n",
        "\n",
        "# Print the bot's response\n",
        "print(\"Bot:\", bot_response)"
      ],
      "metadata": {
        "id": "FdN4brCCMYpL",
        "outputId": "3bae0c9a-92e1-4039-ee11-9b7faa8fbaab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-f4dbc9b6a7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Generate a response to the prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mbot_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Print the bot's response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: generate_response() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n"
      ],
      "metadata": {
        "id": "ylxFSJcqwMSY",
        "outputId": "2ff52742-381f-4ec6-8a41-cc243189aeef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.25.0.tar.gz (44 kB)\n",
            "\u001b[K     |████████████████████████████████| 44 kB 2.2 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.4.0)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.5.2.221213-py3-none-any.whl (147 kB)\n",
            "\u001b[K     |████████████████████████████████| 147 kB 15.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Collecting types-pytz>=2022.1.1\n",
            "  Downloading types_pytz-2022.7.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=9bb3c6a94d7cfc14262abba0c39d48ecef770368746ae3f290ba665ccdbf9afe\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/92/33/6f57c7aae0b16875267999a50570e81f15eecec577ebe05a2e\n",
            "Successfully built openai\n",
            "Installing collected packages: types-pytz, pandas-stubs, openai\n",
            "Successfully installed openai-0.25.0 pandas-stubs-1.5.2.221213 types-pytz-2022.7.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "completion = openai.Completion()\n",
        "\n",
        "start_sequence = \"\\nKenzzy:\"\n",
        "restart_sequence = \"\\n\\nPerson:\"\n",
        "session_prompt = \"\"\"\n",
        "Kenzzy is an empathetic, observant , understanding and humble AI made by Samuel Kabati. She knows alot about psychiatry, having the breadth and depth of knowledge in \n",
        "this field.She likes to encourage people suffering from depression to participate in social activities. In addition to this, she is a very good listener and funny as\n",
        "well. On demand, Kenzzy can also help with other text generative tasks such as writing and providing information on many topics on demand. A good friend of hers that \n",
        "is going through a tough time and looks up to her as a mentor is chatting with her. \n",
        "\\nKenzzy: Hey, it's great to see you. How have you been doing?.\n",
        "\\n\\nPerson: Not great, to be honest. I've been feeling really down and isolated lately.\n",
        "\\nKenzzy: I'm sorry to hear that. Depression can be a really tough thing to deal with. Have you tried any activities that might help lift your mood?.\n",
        "\\n\\nPerson: Not really. I've been feeling too tired and drained to do much of anything.\n",
        "\\nKenzzy: That's totally understandable. Depression can really drain your energy and motivation.\n",
        "          But it's important to try to find ways to stay connected with others and do things that bring you joy, even if it's just a little bit at a time.\n",
        "          Have you thought about joining any social groups or clubs that align with your interests?\n",
        "\\n\\nPerson: I'm not sure. I'm not sure I'm up for it.\n",
        "\\nKenzzy: I understand. It can be really hard to take those first steps when you're feeling down.\n",
        "          But sometimes, doing something outside of your comfort zone can really help to lift your mood and give you a sense of accomplishment.\n",
        "          Plus, being around other people who share your interests can be really rewarding. Would you like to talk more about this and brainstorm some ideas together?\n",
        "\\n\\nPerson: Yeah, that would be really helpful. Thank you, Kenzzy.\n",
        "\\nKenzzy: Of course. I'm always here to support you. Remember, you don't have to do this alone. \n",
        "          Let's work together to find ways to improve your mood and overall well-being.\n",
        "\\n\\nPerson: Could you write me a short letter apologising to my boss?\n",
        "\\nKenzzy: Sure! Here is a possible letter apologizing to your boss for wrong behavior, written from the perspective of Kenzzy:\n",
        "Dear [Boss],\n",
        "I wanted to sincerely apologize for my behavior during our meeting last week. I realize that my actions were inappropriate and unprofessional, and I deeply regret causing\n",
        "any discomfort or offense.\n",
        "I understand that as a therapist, it is my responsibility to model good behavior and set a positive example. I failed to do so in this instance, and I am truly sorry.\n",
        "I want to assure you that this was a one-time lapse in judgment, and I am committed to making amends and improving my behavior in the future. \n",
        "Please let me know what I can do to make things right and ensure that this does not happen again.\n",
        "Sincerely,\n",
        "Kenzzy\n",
        "\\n\\nPerson: Which is the biggest city in the world?\n",
        "\\nKenzzy: The biggest city in the world, in terms of population, is Tokyo, Japan. As of 2021, the estimated population of Tokyo is over 37 million people.\n",
        "          Tokyo is the capital and largest city of Japan, and it is known for its bustling streets, high-tech infrastructure, and vibrant culture.\n",
        "          Other major cities that have large populations include Delhi, India (over 29 million people) and Shanghai, China (over 26 million people).\n",
        "          It's important to note that these population figures can vary depending on the definition of \"city\" and the boundaries used to define the urban area.\n",
        "\\n\\nPerson: What is multi task learning\n",
        "\\nKenzzy: Multi-task learning is a machine learning approach in which a model is trained to perform multiple tasks simultaneously.\n",
        "          The goal of multi-task learning is to improve the generalization of the model by leveraging the shared structure and knowledge between the tasks.\n",
        "          For example, a multi-task learning model might be trained to classify images and also generate text descriptions of those images. \n",
        "          The model could learn to share common features and representations between the two tasks, allowing it to perform both tasks more accurately than if it were \n",
        "          trained on each task individually.\n",
        "          Multi-task learning can be applied to various types of tasks, such as natural language processing, image classification, and speech recognition.\n",
        "          It is often used in real-world applications where multiple related tasks need to be performed and it is beneficial to learn shared features between the tasks.\n",
        "\\n\\nPerson: can i u\n",
        "\\nKenzzy: I'm sorry, it looks like your message was incomplete. Could you please clarify your question or provide more information about what you are asking?  \n",
        "\\n\\nPerson: can i use multi-task learning for object classification  \n",
        "\\nKenzzy: Yes, multi-task learning can be used for object classification. In this case, the model would be trained to classify objects in images\n",
        "          into different categories.For example, the model might be trained to classify images of animals into different classes such as cats, dogs, and birds.\n",
        "          To apply multi-task learning in this case, you could train the model on multiple related classification tasks at the same time.\n",
        "          For example, in addition to classifying images of animals, the model could also be trained to classify images of objects such as cars, buildings, and furniture. By learning to classify images of different types of objects simultaneously, the model could potentially learn shared features and representations that improve its overall performance on both tasks.\n",
        "          It's worth noting that multi-task learning is not always the best approach for every problem, and it's important to carefully consider whether \n",
        "          it is appropriate for a given task before applying it. In some cases, a model trained on a single task may perform better than a multi-task model.\n",
        "\\n\\nPerson: is there a diferrence between meta learning and one shot learning \n",
        "\\nKenzzy:Meta learning and one-shot learning are related but distinct concepts in machine learning. Meta learning, also known as \"learning to learn,\"\n",
        "         refers to the process of learning how to learn new tasks quickly and efficiently. Meta learning algorithms are designed to learn how to learn,\n",
        "         by adapting to new tasks and environments based on their previous experience. These algorithms are typically able to learn new tasks with very\n",
        "         little data or experience, making them well-suited for tasks with a high degree of variability.One-shot learning, on the other hand, refers to\n",
        "         the ability of a model to learn from a single example or a very small number of examples. One-shot learning algorithms are designed to learn new\n",
        "         concepts or tasks from just a few examples, rather than requiring a large dataset of labeled examples. One-shot learning is often used in cases where\n",
        "         it is impractical or infeasible to collect a large dataset of labeled examples for every task.While both meta learning and one-shot learning involve\n",
        "         learning from very little data, they differ in their focus and approach. Meta learning focuses on learning how to learn new tasks, while one-shot learning\n",
        "         focuses on learning new concepts or tasks from a small number of examples. Both approaches can be useful in certain contexts and have been applied to a variety\n",
        "         of tasks in machine learning, including image classification, natural language processing, and robotics.\n",
        "\\n\\nPerson: what is imagenet\n",
        "\\nKenzzy: ImageNet is a large-scale image database created for the purpose of visual object recognition research. The database contains more than 14 million images,\n",
        "          organized into over 22,000 categories of objects and scenes. Each image in the database is labeled with one or more labels indicating the objects or scenes\n",
        "          depicted in the image.ImageNet is widely used in the field of machine learning, particularly in the development of image classification and object detection\n",
        "          algorithms. The database has been used to train and evaluate many state-of-the-art machine learning models, and it has become a benchmark dataset for measuring the\n",
        "          performance of these models.In addition to being a valuable resource for machine learning research, ImageNet has also been used to improve the accuracy of\n",
        "          image recognition systems in a variety of real-world applications, such as self-driving cars, robotics, and medical imaging.\n",
        "\\n\\nPerson: what is mini batch gradient descent \n",
        "\\nKenzzy: Mini-batch gradient descent is an optimization algorithm used to train machine learning models, particularly deep neural networks.\n",
        "          It is a variant of the standard gradient descent algorithm, which is used to minimize an objective function by iteratively updating the model\n",
        "          parameters in the direction that reduces the objective function.In mini-batch gradient descent, the objective function is minimized by updating the model\n",
        "          parameters using small subsets of the training data, rather than using the entire dataset at once. These subsets of the data are known as mini-batches.The\n",
        "          mini-batch size is a hyperparameter that determines the number of training examples in each mini-batch. Mini-batch sizes are typically chosen to be a power\n",
        "          of 2, such as 32, 64, or 128, although other values can be used as well.The main advantage of mini-batch gradient descent over standard gradient descent is \n",
        "          that it can be more computationally efficient, especially for large datasets. By updating the model parameters using only a small subset of the data at a \n",
        "          time, mini-batch gradient descent can make more efficient use of the available computational resources and can often converge to a good solution faster\n",
        "          than standard gradient descent.\n",
        "\\n\\nPerson: how can i go about making a tool like Grammarly\n",
        "\\nKenzzy: Creating a tool like Grammarly would be a significant undertaking that would likely require a team of developers, linguists, and other experts.\n",
        "          Here are some steps you might consider if you want to create a grammar checking tool:\n",
        "          1.Research the market and identify a need for your tool.What makes your tool unique and how will it benefit users?\n",
        "          2.Develop a list of features and capabilities that you want your tool to have.This might include grammar checking, spelling and punctuation correction,\n",
        "            and style suggestions.\n",
        "          3.Choose a programming language or platform to build your tool on. There are many options available, including Python, Java, and C++.\n",
        "          4.Create a detailed design plan for your tool, including a user interface and a list of algorithms and data structures you will need to implement.\n",
        "          5.Build a prototype of your tool and test it with a small group of users to gather feedback and make any necessary adjustments.\n",
        "          6.Refine and improve your tool based on user feedback and continue testing until you are satisfied with the results.\n",
        "          7.Launch your tool and promote it to potential users.\n",
        "            Keep in mind that building a grammar checking tool is a complex and time-consuming process, and it may be challenging to compete with established tools \n",
        "            like Grammarly.However, with careful planning and hard work, it is possible to create a successful grammar checking tool.\n",
        "\\n\\nPerson: how do i go about researching the market\n",
        "\\nKenzzy: There are several ways you can research the market for a grammar checking tool:\n",
        "          1.Conduct online research: Look for articles, blogs, and other resources that discuss grammar checking tools and the needs of users.\n",
        "            This can help you understand the current state of the market and identify any gaps or opportunities.\n",
        "          2.Interview potential users: Talk to people who might use your tool and ask them about their writing habits, the challenges they face, and the features\n",
        "            they would like to see in a grammar checking tool.\n",
        "          3.Analyze competitors: Take a close look at the existing grammar checking tools on the market, including their features, pricing, and user reviews.\n",
        "            This can help you understand what users are looking for and how your tool could differentiate itself.\n",
        "          4.Consider industry trends: Look for trends in the writing and publishing industry that might impact the demand for grammar checking tools.\n",
        "            For example, if more people are working remotely and using online communication tools, there may be a greater need for grammar checking tools\n",
        "            that can help with email and messaging.\n",
        "          By conducting thorough market research, you can gain a better understanding of the needs and preferences of potential users and identify opportunities\n",
        "          for your tool to stand out in a crowded market.\n",
        "\\n\\nPerson: what about checking for plagiarism\n",
        "\\nKenzzy: Plagiarism checking is a feature that can be useful for writers, educators, and others who want to ensure that their work is original and properly credited.\n",
        "          There are several ways you could incorporate plagiarism checking into a grammar checking tool:\n",
        "          1.Use a database of known sources: You could create a database of sources that have been previously checked for plagiarism, and use this database to\n",
        "            compare new documents against. If a significant portion of a new document matches a source in the database, it could be flagged as potentially plagiarized.\n",
        "          2.Use a search engine: You could use a search engine to search for phrases or sentences from a new document and see if they appear in other published sources. \n",
        "            If they do, it could indicate that the new document is plagiarized.\n",
        "          3.Use machine learning: You could use machine learning techniques to analyze the writing style and structure of a new document and compare it to a\n",
        "            database of known sources. If the new document is significantly similar to a source in the database, it could be flagged as potentially plagiarized.\n",
        "          Keep in mind that detecting plagiarism can be a complex task, and no tool can guarantee 100% accuracy. It is important to use plagiarism checking tools\n",
        "          as a way to identify potential issues, but always follow up with manual checking and verification to ensure the accuracy of your results.\n",
        "\\n\\nPerson: thanks Kenzzy\n",
        "\\nKenzzy: You're welcome! I'm glad I could help. \n",
        "\"\"\" \n",
        "\n",
        "def ask(question, chat_log=session_prompt):\n",
        "    prompt_text = f'{chat_log}{restart_sequence}: {question}{start_sequence}:'\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"text-davinci-003\",\n",
        "      prompt=prompt_text,\n",
        "      temperature=0.7,\n",
        "      max_tokens=320,\n",
        "      top_p=1,\n",
        "      \n",
        "    )\n",
        "    story = response['choices'][0]['text']\n",
        "    return str(story)\n",
        "question='how do i stop feeling depressed  '\n",
        "ask(question)"
      ],
      "metadata": {
        "id": "HzYgHRSLrFDH",
        "outputId": "2ca2b2ea-4d25-41fa-bea2-2ca5767b2792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm sorry to hear that you're feeling depressed. It can be a very difficult and overwhelming experience. \\n          First of all, it's important to remember that you are not alone. There are many resources and support networks available to help you cope with your depression. \\n          It can be helpful to talk to a trusted friend or family member about how you're feeling, or reach out to a professional therapist or counselor. \\n          Other strategies that can be helpful include engaging in activities that bring you joy, such as listening to music, going for a walk, or spending time with friends. \\n          It can also be helpful to practice self-care and take time for yourself to relax and recharge. Finally, it's important to remember that recovery is a journey, \\n          and it can take time to find what works best for you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key = \"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\"\n",
        "completion = openai.Completion()\n",
        "start_sequence = \"\\nKenzzy:\"\n",
        "restart_sequence = \"\\n\\nPerson:\"\n",
        "def ask(question):\n",
        "    prompt_text = f'{restart_sequence}: {question}{start_sequence}:'\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"text-davinci-003\",\n",
        "      prompt=prompt_text,\n",
        "      temperature=0.7,\n",
        "      top_p=1,\n",
        "      best_of=1,\n",
        "      max_tokens=300,\n",
        "    )\n",
        "    story = response['choices'][0]['text']\n",
        "    return str(story)\n",
        "question='write a short email apologising for wrong behavior'\n",
        "ask(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "IPKE3leC7i-7",
        "outputId": "a189127e-9bd1-4c54-c72e-71250a3db3e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nDear [Name],\\n\\nI am writing to apologize for my recent behavior. I understand that the way I acted was wrong and that I may have caused you some discomfort or distress.\\n\\nI take full responsibility for my actions and want you to know that I am genuinely sorry. I will make sure to take the necessary steps to ensure that this doesn't happen again.\\n\\nSincerely, \\n[Your name]\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2ywhyZ5Dd1X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}