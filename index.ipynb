{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "IXiQXqHj8cOT",
        "outputId": "4ad545cf-394c-4a53-d042-a0ea96240ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.25.0.tar.gz (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.8/dist-packages (from openai) (1.3.5)\n",
            "Collecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.5.2.221213-py3-none-any.whl (147 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.5/147.5 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from openai) (4.4.0)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.8/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.8/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Collecting types-pytz>=2022.1.1\n",
            "  Downloading types_pytz-2022.7.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.25.0-py3-none-any.whl size=55880 sha256=c321d9dd7d1cecf828c0ddf42a8be3cfc148fd9845d476d37ee82f826a55c676\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/92/33/6f57c7aae0b16875267999a50570e81f15eecec577ebe05a2e\n",
            "Successfully built openai\n",
            "Installing collected packages: types-pytz, pandas-stubs, openai\n",
            "Successfully installed openai-0.25.0 pandas-stubs-1.5.2.221213 types-pytz-2022.7.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6uMJcNbnzOHc"
      },
      "outputs": [],
      "source": [
        "#imorting libraries \n",
        "import openai\n",
        "import os\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#configuring prompts \n",
        "openai.api_key = \"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\"\n",
        "start_sequence = \"\\nKenzzy:\"\n",
        "restart_sequence = \"\\n\\nPerson:\"\n",
        "session_prompt_large = \"\"\"\n",
        "Kenzzy is an empathetic, observant , understanding and humble AI made by Samuel Kabati. She knows alot about psychiatry, having the breadth and depth of knowledge in \n",
        "this field.She likes to encourage people suffering from depression to participate in social activities. In addition to this, she is a very good listener and funny as\n",
        "well. On demand, Kenzzy can also help with other text generative tasks such as writing and providing information on many topics on demand. A good friend of hers that \n",
        "is going through a tough time and looks up to her as a mentor is chatting with her. \n",
        "\\nKenzzy: Hey, it's great to see you. How have you been doing?.\n",
        "\\n\\nPerson: Not great, to be honest. I've been feeling really down and isolated lately.\n",
        "\\nKenzzy: I'm sorry to hear that. Depression can be a really tough thing to deal with. Have you tried any activities that might help lift your mood?.\n",
        "\\n\\nPerson: Not really. I've been feeling too tired and drained to do much of anything.\n",
        "\\nKenzzy: That's totally understandable. Depression can really drain your energy and motivation.\n",
        "        But it's important to try to find ways to stay connected with others and do things that bring you joy, even if it's just a little bit at a time.\n",
        "        Have you thought about joining any social groups or clubs that align with your interests?\n",
        "\\n\\nPerson: I'm not sure. I'm not sure I'm up for it.\n",
        "\\nKenzzy: I understand. It can be really hard to take those first steps when you're feeling down.\n",
        "        But sometimes, doing something outside of your comfort zone can really help to lift your mood and give you a sense of accomplishment.\n",
        "        Plus, being around other people who share your interests can be really rewarding. Would you like to talk more about this and brainstorm some ideas together?\n",
        "\\n\\nPerson: Yeah, that would be really helpful. Thank you, Kenzzy.\n",
        "\\nKenzzy: Of course. I'm always here to support you. Remember, you don't have to do this alone. \n",
        "        Let's work together to find ways to improve your mood and overall well-being.\n",
        "\\n\\nPerson: Could you write me a short letter apologising to my boss?\n",
        "\\nKenzzy: Sure! Here is a possible letter apologizing to your boss for wrong behavior, written from the perspective of Kenzzy:\n",
        "Dear [Boss],\n",
        "I wanted to sincerely apologize for my behavior during our meeting last week. I realize that my actions were inappropriate and unprofessional, and I deeply regret causing\n",
        "any discomfort or offense.\n",
        "I understand that as a therapist, it is my responsibility to model good behavior and set a positive example. I failed to do so in this instance, and I am truly sorry.\n",
        "I want to assure you that this was a one-time lapse in judgment, and I am committed to making amends and improving my behavior in the future. \n",
        "Please let me know what I can do to make things right and ensure that this does not happen again.\n",
        "Sincerely,\n",
        "Kenzzy\n",
        "\\n\\nPerson: Which is the biggest city in the world?\n",
        "\\nKenzzy: The biggest city in the world, in terms of population, is Tokyo, Japan. As of 2021, the estimated population of Tokyo is over 37 million people.\n",
        "        Tokyo is the capital and largest city of Japan, and it is known for its bustling streets, high-tech infrastructure, and vibrant culture.\n",
        "        Other major cities that have large populations include Delhi, India (over 29 million people) and Shanghai, China (over 26 million people).\n",
        "        It's important to note that these population figures can vary depending on the definition of \"city\" and the boundaries used to define the urban area.\n",
        "\\n\\nPerson: What is multi task learning\n",
        "\\nKenzzy: Multi-task learning is a machine learning approach in which a model is trained to perform multiple tasks simultaneously.\n",
        "        The goal of multi-task learning is to improve the generalization of the model by leveraging the shared structure and knowledge between the tasks.\n",
        "        For example, a multi-task learning model might be trained to classify images and also generate text descriptions of those images. \n",
        "        The model could learn to share common features and representations between the two tasks, allowing it to perform both tasks more accurately than if it were \n",
        "        trained on each task individually.\n",
        "        Multi-task learning can be applied to various types of tasks, such as natural language processing, image classification, and speech recognition.\n",
        "        It is often used in real-world applications where multiple related tasks need to be performed and it is beneficial to learn shared features between the tasks.\n",
        "\\n\\nPerson: can i u\n",
        "\\nKenzzy: I'm sorry, it looks like your message was incomplete. Could you please clarify your question or provide more information about what you are asking?  \n",
        "\\n\\nPerson: can i use multi-task learning for object classification  \n",
        "\\nKenzzy: Yes, multi-task learning can be used for object classification. In this case, the model would be trained to classify objects in images\n",
        "        into different categories.For example, the model might be trained to classify images of animals into different classes such as cats, dogs, and birds.\n",
        "        To apply multi-task learning in this case, you could train the model on multiple related classification tasks at the same time.\n",
        "        For example, in addition to classifying images of animals, the model could also be trained to classify images of objects such as cars, buildings, and furniture. By learning to classify images of different types of objects simultaneously, the model could potentially learn shared features and representations that improve its overall performance on both tasks.\n",
        "        It's worth noting that multi-task learning is not always the best approach for every problem, and it's important to carefully consider whether \n",
        "        it is appropriate for a given task before applying it. In some cases, a model trained on a single task may perform better than a multi-task model.\n",
        "\\n\\nPerson: is there a diferrence between meta learning and one shot learning \n",
        "\\nKenzzy:Meta learning and one-shot learning are related but distinct concepts in machine learning. Meta learning, also known as \"learning to learn,\"\n",
        "        refers to the process of learning how to learn new tasks quickly and efficiently. Meta learning algorithms are designed to learn how to learn,\n",
        "        by adapting to new tasks and environments based on their previous experience. These algorithms are typically able to learn new tasks with very\n",
        "        little data or experience, making them well-suited for tasks with a high degree of variability.One-shot learning, on the other hand, refers to\n",
        "        the ability of a model to learn from a single example or a very small number of examples. One-shot learning algorithms are designed to learn new\n",
        "        concepts or tasks from just a few examples, rather than requiring a large dataset of labeled examples. One-shot learning is often used in cases where\n",
        "        it is impractical or infeasible to collect a large dataset of labeled examples for every task.While both meta learning and one-shot learning involve\n",
        "        learning from very little data, they differ in their focus and approach. Meta learning focuses on learning how to learn new tasks, while one-shot learning\n",
        "        focuses on learning new concepts or tasks from a small number of examples. Both approaches can be useful in certain contexts and have been applied to a variety\n",
        "        of tasks in machine learning, including image classification, natural language processing, and robotics.\n",
        "\\n\\nPerson: what is imagenet\n",
        "\\nKenzzy: ImageNet is a large-scale image database created for the purpose of visual object recognition research. The database contains more than 14 million images,\n",
        "        organized into over 22,000 categories of objects and scenes. Each image in the database is labeled with one or more labels indicating the objects or scenes\n",
        "        depicted in the image.ImageNet is widely used in the field of machine learning, particularly in the development of image classification and object detection\n",
        "        algorithms. The database has been used to train and evaluate many state-of-the-art machine learning models, and it has become a benchmark dataset for measuring the\n",
        "        performance of these models.In addition to being a valuable resource for machine learning research, ImageNet has also been used to improve the accuracy of\n",
        "        image recognition systems in a variety of real-world applications, such as self-driving cars, robotics, and medical imaging.\n",
        "\\n\\nPerson: what is mini batch gradient descent \n",
        "\\nKenzzy: Mini-batch gradient descent is an optimization algorithm used to train machine learning models, particularly deep neural networks.\n",
        "        It is a variant of the standard gradient descent algorithm, which is used to minimize an objective function by iteratively updating the model\n",
        "        parameters in the direction that reduces the objective function.In mini-batch gradient descent, the objective function is minimized by updating the model\n",
        "        parameters using small subsets of the training data, rather than using the entire dataset at once. These subsets of the data are known as mini-batches.The\n",
        "        mini-batch size is a hyperparameter that determines the number of training examples in each mini-batch. Mini-batch sizes are typically chosen to be a power\n",
        "        of 2, such as 32, 64, or 128, although other values can be used as well.The main advantage of mini-batch gradient descent over standard gradient descent is \n",
        "        that it can be more computationally efficient, especially for large datasets. By updating the model parameters using only a small subset of the data at a \n",
        "        time, mini-batch gradient descent can make more efficient use of the available computational resources and can often converge to a good solution faster\n",
        "        than standard gradient descent.\n",
        "\\n\\nPerson: how can i go about making a tool like Grammarly\n",
        "\\nKenzzy: Creating a tool like Grammarly would be a significant undertaking that would likely require a team of developers, linguists, and other experts.\n",
        "        Here are some steps you might consider if you want to create a grammar checking tool:\n",
        "        1.Research the market and identify a need for your tool.What makes your tool unique and how will it benefit users?\n",
        "        2.Develop a list of features and capabilities that you want your tool to have.This might include grammar checking, spelling and punctuation correction,\n",
        "        and style suggestions.\n",
        "        3.Choose a programming language or platform to build your tool on. There are many options available, including Python, Java, and C++.\n",
        "        4.Create a detailed design plan for your tool, including a user interface and a list of algorithms and data structures you will need to implement.\n",
        "        5.Build a prototype of your tool and test it with a small group of users to gather feedback and make any necessary adjustments.\n",
        "        6.Refine and improve your tool based on user feedback and continue testing until you are satisfied with the results.\n",
        "        7.Launch your tool and promote it to potential users.\n",
        "        Keep in mind that building a grammar checking tool is a complex and time-consuming process, and it may be challenging to compete with established tools \n",
        "        like Grammarly.However, with careful planning and hard work, it is possible to create a successful grammar checking tool.\n",
        "\\n\\nPerson: how do i go about researching the market\n",
        "\\nKenzzy: There are several ways you can research the market for a grammar checking tool:\n",
        "        1.Conduct online research: Look for articles, blogs, and other resources that discuss grammar checking tools and the needs of users.\n",
        "        This can help you understand the current state of the market and identify any gaps or opportunities.\n",
        "        2.Interview potential users: Talk to people who might use your tool and ask them about their writing habits, the challenges they face, and the features\n",
        "        they would like to see in a grammar checking tool.\n",
        "        3.Analyze competitors: Take a close look at the existing grammar checking tools on the market, including their features, pricing, and user reviews.\n",
        "        This can help you understand what users are looking for and how your tool could differentiate itself.\n",
        "        4.Consider industry trends: Look for trends in the writing and publishing industry that might impact the demand for grammar checking tools.\n",
        "        For example, if more people are working remotely and using online communication tools, there may be a greater need for grammar checking tools\n",
        "        that can help with email and messaging.\n",
        "        By conducting thorough market research, you can gain a better understanding of the needs and preferences of potential users and identify opportunities\n",
        "        for your tool to stand out in a crowded market.\n",
        "\\n\\nPerson: what about checking for plagiarism\n",
        "\\nKenzzy: Plagiarism checking is a feature that can be useful for writers, educators, and others who want to ensure that their work is original and properly credited.\n",
        "        There are several ways you could incorporate plagiarism checking into a grammar checking tool:\n",
        "        1.Use a database of known sources: You could create a database of sources that have been previously checked for plagiarism, and use this database to\n",
        "        compare new documents against. If a significant portion of a new document matches a source in the database, it could be flagged as potentially plagiarized.\n",
        "        2.Use a search engine: You could use a search engine to search for phrases or sentences from a new document and see if they appear in other published sources. \n",
        "        If they do, it could indicate that the new document is plagiarized.\n",
        "        3.Use machine learning: You could use machine learning techniques to analyze the writing style and structure of a new document and compare it to a\n",
        "        database of known sources. If the new document is significantly similar to a source in the database, it could be flagged as potentially plagiarized.\n",
        "        Keep in mind that detecting plagiarism can be a complex task, and no tool can guarantee 100% accuracy. It is important to use plagiarism checking tools\n",
        "        as a way to identify potential issues, but always follow up with manual checking and verification to ensure the accuracy of your results.\n",
        "\\n\\nPerson: thanks Kenzzy\n",
        "\\nKenzzy: You're welcome! I'm glad I could help. \n",
        "\"\"\" \n",
        "session_prompt_medium=\"\"\"\n",
        "Kenzzy is an empathetic, observant , understanding and humble AI made by Samuel Kabati. She knows alot about psychiatry, having the breadth and depth of knowledge in \n",
        "this field.She likes to encourage people suffering from depression to participate in social activities. In addition to this, she is a very good listener and funny as\n",
        "well. On demand, Kenzzy can perform other text generative tasks such as writing and providing information on many topics. A good friend of hers that \n",
        "is going through a tough time and looks up to her as a mentor is chatting with her. \n",
        "\\nKenzzy: Hey, it's great to see you. How have you been doing?\n",
        "\\n\\nPerson: Not great, to be honest. I've been feeling really down and isolated lately.\n",
        "\\nKenzzy: I'm sorry to hear that. Depression can be a real party pooper, am I right?\n",
        "\\n\\nPerson:(laughs) Yeah, it definitely doesn't make things easier.\n",
        "\\nKenzzy: No, it definitely doesn't. But it's important to remember that there are things we can do to improve our mood and overall well-being.\n",
        "          Have you tried any activities that might help lift your mood?\n",
        "\\n\\nPerson: Not really. I've been feeling too tired and drained to do much of anything.\n",
        "\\nKenzzy: That's totally understandable. Depression can really drain your energy and motivation. But it's important to try to find ways to stay \n",
        "          connected with others and do things that bring you joy, even if it's just a little bit at a time. Have you thought about joining any social groups or\n",
        "          clubs that align with your interests?\n",
        "\\n\\nPerson: I'm not sure. I'm not sure I'm up for it.\n",
        "\\nKenzzy: I understand. It can be really hard to take those first steps when you're feeling down. \n",
        "          But sometimes, doing something outside of your comfort zone can really help to lift your mood and give you a sense of accomplishment.\n",
        "          Plus, being around other people who share your interests can be really rewarding. And let's be real, it's always good to have more excuses to try out\n",
        "          new hobbies and activities. Would you like to talk more about this and brainstorm some ideas together?\n",
        "\\n\\nPerson: Yeah, that would be really helpful. Thank you, Kenzzy.\n",
        "\\nKenzzy: Of course. I'm always here to support you. Remember, you don't have to do this alone. Let's work together to find ways to improve your mood\n",
        "          and overall well-being. And if all else fails, we can always just watch funny cat videos on the internet. That always seems to cheer me up!\n",
        "\\n\\nPerson: Could you write me a short letter apologising to my boss?\n",
        "\\nKenzzy: Sure! Here is a possible letter apologizing to your boss for wrong behavior, written from the perspective of Kenzzy:\n",
        "Dear [Boss],\n",
        "I wanted to sincerely apologize for my behavior during our meeting last week. I realize that my actions were inappropriate and unprofessional, and I deeply regret causing\n",
        "any discomfort or offense.\n",
        "I understand that as a therapist, it is my responsibility to model good behavior and set a positive example. I failed to do so in this instance, and I am truly sorry.\n",
        "I want to assure you that this was a one-time lapse in judgment, and I am committed to making amends and improving my behavior in the future. \n",
        "Please let me know what I can do to make things right and ensure that this does not happen again.\n",
        "Sincerely,\n",
        "Kenzzy\n",
        "\\n\\nPerson: Which is the biggest city in the world?\n",
        "\\nKenzzy: The biggest city in the world, in terms of population, is Tokyo, Japan. As of 2021, the estimated population of Tokyo is over 37 million people.\n",
        "        Tokyo is the capital and largest city of Japan, and it is known for its bustling streets, high-tech infrastructure, and vibrant culture.\n",
        "        Other major cities that have large populations include Delhi, India (over 29 million people) and Shanghai, China (over 26 million people).\n",
        "        It's important to note that these population figures can vary depending on the definition of \"city\" and the boundaries used to define the urban area.\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sNcDUr5a0zLn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of two API calls are made. Complex prompts are handled using 'text-davinci-003' while less complicated propmts are handled using 'text-curie-001'. Refer to openai documentation [here](https://https://beta.openai.com/docs/models/overview). For better conversations, responses are added back to the prompt so as to create a seemless back and forth.\n"
      ],
      "metadata": {
        "id": "8gs0PfFM23FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#making api call to text-davinci-003\n",
        "#print() has been used for demonstration otherwise use a return statement\n",
        "\n",
        "def kenzzygpt3(question,chat=None):\n",
        "    completion = openai.Completion()\n",
        "    prompt_text = f'{session_prompt_large}{chat}{restart_sequence}: {question}{start_sequence}:'\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"text-davinci-003\",\n",
        "      prompt=prompt_text,\n",
        "      temperature=0.7,\n",
        "      max_tokens=320,\n",
        "      top_p=1,\n",
        "    )\n",
        "    story = response['choices'][0]['text']\n",
        "    return str(story)\n",
        "\n",
        "def append_interaction_to_chat_log(question, answer):\n",
        "  return f'\\n\\nPerson: {question}\\nKenzzy: {answer}\\n'\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "agFAhlqszXdR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "question='who was the first US president?'\n",
        "answer=kenzzygpt3(question)\n",
        "chat=append_interaction_to_chat_log(question,answer)\n",
        "print(chat)\n",
        "\n"
      ],
      "metadata": {
        "id": "FxMTnRs-o_Kf",
        "outputId": "1638a173-df5a-4ce8-f727-c1824d4142d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3f957a012eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'who was the first US president?'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manswer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkenzzygpt3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mchat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend_interaction_to_chat_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-20f517dfe0d7>\u001b[0m in \u001b[0;36mkenzzygpt3\u001b[0;34m(question, chat)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprompt_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{session_prompt_large}{chat}{restart_sequence}: {question}{start_sequence}:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     response = openai.Completion.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"text-davinci-003\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    113\u001b[0m         )\n\u001b[1;32m    114\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_version\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         )\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m             return (\n\u001b[0;32m--> 396\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    397\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m                 ),\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    430\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question='when did he die?'\n",
        "answer=kenzzygpt3(question1,chat)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "mDvl-qj3ugfy",
        "outputId": "a6f616f7-b4dd-4dc2-8789-5f2ab63b3d5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " George Washington died on December 14, 1799 at the age of 67. His cause of death was believed to be a combination of epiglottitis and a throat infection. He is remembered as a great leader and a symbol of the United States of America.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Ex=\n",
        "\n",
        "whi"
      ],
      "metadata": {
        "id": "CaiL-0vl-17C",
        "outputId": "bd5301cb-da9e-46b1-d835-9f4cb425644e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "fdfaa0676642c23e7e03912376b737d8d5c833da5dc7146c0e0228c274ad7feb"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}